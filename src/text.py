import warnings

import os
import re
import unicodedata

from transformers import AutoTokenizer, AutoModel
import torch
import faiss

import subprocess

import PyPDF2

def main():   
    warnings.filterwarnings(
        "ignore", 
        category=FutureWarning, 
        message=r"`clean_up_tokenization_spaces` was not set"
    )

    query = input("\n-> Enter the query to search for relevant documents: ")
    cleaned_query = clean_string(query)
    
    pdfs_names = get_pdfs_names("documents")
    cleared_pdfs_names = clear_list(pdfs_names)

    relevant_pdfs = get_relevant_pdfs(cleaned_query, cleared_pdfs_names)

    print("\n-> Documenti rilevanti:")
    for pdf in relevant_pdfs:
        print(pdf)

    

    relevant_pdfs_embeddings = embedding(relevant_pdfs)
    for doc, emb in zip(relevant_pdfs, relevant_pdfs_embeddings):
        print(f"{doc:<40}: {emb[:5]}...")

    #########################
    
    index = faiss.IndexFlatL2(relevant_pdfs_embeddings.shape[1])
    index.add(relevant_pdfs_embeddings)

    cleaned_query_embedding = embedding(cleaned_query)

    distances, indices = index.search(cleaned_query_embedding, min(3,len(relevant_pdfs)))

    retrieve = [(relevant_pdfs[i], distances[0][idx]) for idx, i in enumerate(indices[0])]

    print()

    for doc, dis in retrieve:
        print(f"{doc:<40}: {dis:.4f}")

    print()



    docs_text = " ".join([self.pdf_texts[self.pdf_filenames.index(doc)] for doc, _ in documents if self.pdf_texts[self.pdf_filenames.index(doc)]])[:1000]  # Take only the first 1000 characters
    
    if not docs_text.strip():
        return "No relevant documents found to answer the query."

    prompt = f"Question: {query}\n\nDocuments:\n{docs_text}"
    
    try:
        process = subprocess.Popen(
            ['wsl', 'ollama', 'run', 'codellama'], 
            stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, 
            text=True, encoding='utf-8', errors='replace'
        )

        stdout, stderr = process.communicate(input=prompt)

        if stderr:
            print(f"Error from Ollama: {stderr}")
        
        response =  stdout.strip()

    except Exception as e:
        print(f"An error occurred: {e}")
        return "Failed to generate a response from Ollama."
    
    print("\n-> Response generated by Ollama:\n", response)

######################

def get_pdfs_names(directory):
    pdfs_names = []

    for pdf_name in os.listdir(directory):
        if pdf_name.endswith(".pdf"):
            pdf_name = clean_string(pdf_name)
            pdfs_names.append(pdf_name)

    return pdfs_names

def clear_list(list):
    cleared_list = []
    for string in list:
        cleared_string = clean_string(string)
        cleared_list.append(cleared_string)
    return cleared_list

def clean_string(text):
    text = text.lower()
    text = os.path.splitext(text)[0]
    text = text.replace("_", " ")
    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')
    text = re.sub(r"[^\w\s]", '', text)
    text = remove_single_letters(text)
    text = text.replace("  ", " ").strip()

    return text

def remove_single_letters(text):
    words = text.split()
    filtered_words = [word for word in words if len(word) > 1]
    return ' '.join(filtered_words)

def get_relevant_pdfs(cleaned_query, cleared_pdfs_names):
    relevant_pdfs = []
    query_words = cleaned_query.split()

    for pdf_name in cleared_pdfs_names:
        doc_words = pdf_name.split()

        if any(word in doc_words for word in query_words):
            relevant_pdfs.append(pdf_name)

    return relevant_pdfs

def embedding(strings):
    tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
    model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

    inputs = tokenizer(strings, padding=True, truncation=True, return_tensors="pt")

    with torch.no_grad():
        embeddings = model(**inputs).last_hidden_state.mean(dim=1).numpy()

    faiss.normalize_L2(embeddings)

    return embeddings

def get_pdfs_contents(documents_names):
    documents_contents = []
    for document_name in documents_names:
        content = __extract_content_from_pdf(document_name)
        documents_contents.append(content)
    return documents_contents

# Function to extract text from a single PDF with improvements
def __extract_content_from_pdf(document_directory, document_name):
    content = []  # Use a list to accumulate the content for better performance
    try:
        with open(os.path.join(document_directory, document_name), 'rb') as document:
            pdf_reader = PyPDF2.PdfReader(document)
            
            for page in range(len(pdf_reader.pages)):
                page_text = pdf_reader.pages[page].extract_text()
                
                # Check if text was extracted from the page
                if page_text:
                    content.append(page_text)
        
        # Join the list of text into a single string
        return "".join(content)

    except Exception as e:
        print(f"\n-> Error reading {document_name}: {e}")
        return ""

if __name__ == "__main__":
    main()    
